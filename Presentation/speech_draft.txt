Speech draft

[Slide 1]

Hello everyone, today I will be presenting our project Benchmarking Structural Variant Callers using Whole Genome Sequencing Data.

So what is a structural variant?
- Here, we define a structural variant as any genomic variation of size > 50 bp
So naturally that excludes single-nucleotide variants and other small indels. As you can see, there are many different types of variation that can occur in a genome.
The diagram shown is just a few examples of the types of structural variant one can find in cancer genomes. However, some simpler variants are commonly found in normal genomes as well.

Next, our aim for this project is to benchmark four common structural variant callers, namely Delly, Manta, SvABA, and Wham, against a validated variant set from the genome NA12878, which we will call the "truth set", and NA12878 is the genome we used for this study. In short, we want to know:

"Which structural variant caller or callers perform the best out of the four?"

And we measure their performance by their precision and recall rate.

The information we want to find could be useful in studies where a consensus list of structural variants for a cohort is absent, for example our side-project on exceptional responders, which I will refrain from talking about today.

Our methodology, to keep it short, is to use the raw VCF files from the caller workflow, so from GATK and SvABA over Terra, the cloud platform,

convert them to respective genomic ranges object in R so we get the genomic addresses of the variants that were called,
and we do the same with the validated truth set that we've obtained from Lam et al's study, which has been used in several benchmarking studies using NA12878,
and we define a match between our caller set of variants and the truth set of variants by their breakpoint overlaps. Such a call is counted as a true positive.

As we will see in the next slide, we measure each caller's performace therefore by the total count of true positives they can capture divided by either the total calls they have made or the size of the truth set.

[Slide 2]

First, note that recall is measured by percentage true positive count by a given caller out of the truth set,

and precision is the proportion of true positives out of the total calls made by a given caller.

We'll just look at the 35 ex and 18 ex coverage levels,

we can see that Manta has captured the most true variants out of the four and SvABA has performed with the highest precision at 35 ex. This trend holds even at half the coverage, namely at 18 ex.

We want to also note that the number of calls made by each caller also correlates with their recall rate.
So we can see that each caller has a different style in the way they report variants, which we will take into consideration when using our benchmarking results.

[Slide 3]

And finally, we conclude that 
at 35 ex coverage, Manta produces high call number and recall rate, however with lower precision.
On the contrary, SvABA performed with high precision at 35 ex with lower recall rate.

Both Wham and Delly, on the other hand, could not outperform Manta or SvABA if we consider both their precision and recall rate altogether.

Putting Wham and Delly aside, we were curious if SvABA qualifies variants more stringently than Manta, such that SvABA made only around half of calls as Manta's. So, we looked at the support read information in these callers to see if the variants called by SvABA were more supported.

To our surprise, Manta has an overall higher supporting read count than SvABA as well as a lower variance, despite its lower precision at 35 ex and higher recall from the truth set. So we are unsure why SvABA made less call than Manta at this stage.

We are also interested in how much the calls overlap between callers. To simplify things we selected Manta and SvABA out of the four callers as we are most interested in these two. The distribution is shown on the right.

I think my time is up. Thank you for listening!